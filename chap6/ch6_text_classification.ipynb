{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71d1aec-daae-422c-b741-f66fcf7e2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# [CLS] -> BertModel -> Linear -> tanh -> Linear\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5631f5db-798a-4d67-88f1-2999354318cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee09717-de60-4812-9796-51f3620757b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "text_list = [\n",
    "    \"この映画は面白かった。\",\n",
    "    \"この映画の最後にはがっかりさせられた。\",\n",
    "    \"この映画を見て幸せな気持ちになった。\",\n",
    "]\n",
    "label_list = [\n",
    "    1,\n",
    "    0,\n",
    "    1\n",
    "]\n",
    "\n",
    "# 符号化\n",
    "encoding: BatchEncoding = tokenizer(\n",
    "    text_list,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "labels = torch.tensor(label_list).cuda()\n",
    "\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "# 分類スコア\n",
    "# 形状は(バッチサイズ（文章数）、 カテゴリ数)\n",
    "scores = output.logits\n",
    "labels_predicted = scores.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026e3222-82ea-4398-96fd-065daf5fc7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135a8e46-0f67-4acf-8bbc-3913fdec8b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# モデルの分類器のパラメータ初期値はランダムな値のため、精度は低い\n",
    "num_correct = (labels_predicted == labels).sum().item()\n",
    "accuracy = num_correct / labels.size(0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074aa12b-c652-48bf-b5b6-fe8bf4eaab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 符号化\n",
    "encoding: BatchEncoding = tokenizer(\n",
    "    text_list,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "encoding[\"labels\"] = torch.tensor(label_list)\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "# ロスの計算\n",
    "output = bert_sc(**encoding)\n",
    "loss = output.loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe28c5-0f25-4937-aed3-ced6602cf7b4",
   "metadata": {},
   "source": [
    "OSコマンドで livedoor ニュースコーパスをダウンロード後、以下の処理につづく"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43784e45-e5ff-4cce-b2d5-784739faed84",
   "metadata": {},
   "source": [
    "## 6-5 BERT のファインチューニングと性能評価\n",
    "- データ(符号化された文章)とラベルを抜き出し、ミニバッチにする\n",
    "- データローダはデータセットからミニバッチを取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a72c833-4fa6-4679-b71d-70245beb038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "{'data': tensor([[0, 1],\n",
      "        [2, 3]]), 'labels': tensor([0, 1])}\n",
      "batch 1\n",
      "{'data': tensor([[4, 5],\n",
      "        [6, 7]]), 'labels': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "dataset_for_loader = [\n",
    "    {\n",
    "        \"data\": torch.tensor([0, 1]),\n",
    "        \"labels\": torch.tensor(0),\n",
    "    },\n",
    "    {\n",
    "        \"data\": torch.tensor([2, 3]),\n",
    "        \"labels\": torch.tensor(1),\n",
    "    },\n",
    "    {\n",
    "        \"data\": torch.tensor([4, 5]),\n",
    "        \"labels\": torch.tensor(2),\n",
    "    },\n",
    "    {\n",
    "        \"data\": torch.tensor([6, 7]),\n",
    "        \"labels\": torch.tensor(3),\n",
    "    },\n",
    "]\n",
    "loader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# ミニバッチを取り出す\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f\"batch {idx}\")\n",
    "    print(batch)\n",
    "    # ファインチューニングではここでミニバッチ毎の処理をおこなう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b22196-ea4c-48ec-9a7a-9f06f5b22bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "{'data': tensor([[4, 5],\n",
      "        [6, 7]]), 'labels': tensor([2, 3])}\n",
      "batch 1\n",
      "{'data': tensor([[0, 1],\n",
      "        [2, 3]]), 'labels': tensor([0, 1])}\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset_for_loader, batch_size=2, shuffle=True)\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f\"batch {idx}\")\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab036a-b397-49cb-8ac2-116c8c6a1571",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "各データを次のキーを持つ辞書にする。\n",
    "\n",
    "すなわち、 tokenizer で符号化を行ったときに得られる形式\n",
    "\n",
    "- input_ids\n",
    "- attention_mask\n",
    "- token_type_ids\n",
    "- labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff5e1eb-2ce1-4c04-9751-1618915503ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:17<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "category_list = [\n",
    "    \"dokujo-tsushin\",\n",
    "    \"it-life-hack\",\n",
    "    \"kaden-channel\",\n",
    "    \"livedoor-homme\",\n",
    "    \"movie-enter\",\n",
    "    \"peachy\",\n",
    "    \"smax\",\n",
    "    \"sports-watch\",\n",
    "    \"topic-news\",\n",
    "]\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データ整形\n",
    "# 学習の高速化のため128とする\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "\n",
    "for label, category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f\"./text/{category}/{category}*\"):\n",
    "        lines = open(file, encoding=\"utf-8\").read().splitlines()\n",
    "        # 4行目からが本文\n",
    "        text = \"\\n\".join(lines[3:])\n",
    "        encoding = tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        encoding[\"labels\"] = label\n",
    "        # なぜこの場合はここでtensor化するのか? return_tensor ではだめなのか?\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325edba7-c6bf-4544-8e32-9e42b7e69870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]),\n",
      " 'input_ids': tensor([    2,  2340, 19693, 10585, 28459,    35,  6692, 28493,    13,   501,\n",
      "           62,   101,    37,     8,   569,   335,     5,    51,     7,     9,\n",
      "         1040,     5,   616,     9,  2941,    18,  5602,   501,    20,    16,\n",
      "         4027, 10531,   140,    36,    73, 30020, 28457, 25127,    38,  1080,\n",
      "            5,    53,    28,   707,     5,    12,     9,    80,  3635,   205,\n",
      "           29,  2935,   604,  5846,  6503,    11,  4722,    16,   861,    13,\n",
      "            6, 12272, 24050,  2079,    11,    26,    62,    45,    28,  2451,\n",
      "           80,     8,    36, 24050,    14,    31,  1058,    75, 11218, 10531,\n",
      "         3676,   542,     5, 22130,     6,  5408,    16,  4831,    80,    29,\n",
      "           18,  7045,    26, 28456,  4799,   900,     6,   569,   335,     9,\n",
      "         1704,  1277,    15,  3318,  2575,    29,  2935,  5233,    75,    13,\n",
      "         3472,   459,    12,  8585,  3171,   312,  3676,   542, 22130,   241,\n",
      "            5,   709, 28696,  2180,    14, 12959,    71,     3]),\n",
      " 'labels': tensor(0),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "7367\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset_for_loader[0])\n",
    "print(len(dataset_for_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d3cbd1-59f6-41fc-900e-0ecf8ae6a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット分割\n",
    "# 学習/検証/テスト = 6:2:2 で分割\n",
    "random.shuffle(dataset_for_loader)\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "\n",
    "dataset_train = dataset_for_loader[:n_train]\n",
    "dataset_val = dataset_for_loader[n_train: n_train + n_val]\n",
    "dataset_test = dataset_for_loader[n_train + n_val:]\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# 学習はshuffle=True\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    # BERTのオリジナルの論文を参考に32としているとのこと\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    # 損失の勾配を計算しないので、大きめのバッチサイズ\n",
    "    batch_size=256,\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092bdbf-05bc-47c3-9554-63158eded0b7",
   "metadata": {},
   "source": [
    "## Pytorch Lightning\n",
    "- PyTorchで書くこともできるが、Lightning はモデルやデータによらず共通の処理が内部であらかじめ実装されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6176d553-c77f-4c21-b890-41d6d431435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの振る舞いを記述するクラス\n",
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        lr: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 例えば、 self.hparams.lr にアクセス可能. 自動で引数が設定される\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # 学習データのミニバッチが与えられたときに損失を出力する関数を書く\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証データのミニバッチが与えられたときに検証データの評価指標を計算する関数を書く\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    # テストデータのミニバッチが与えられたときにテストデータの評価指標を計算する関数を書く\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4090b53-2a44-40b8-b858-f5c5e28f1512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\ykite\\anaconda3\\envs\\bert_book_stockmark\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath=\"model/\"\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=\"auto\",\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cef7577c-a897-49aa-8cb5-325e12907d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: C:\\Users\\ykite\\Documents\\projects\\study\\bert_book_stockmark\\chap6\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | bert_sc | BertForSequenceClassification | 110 M \n",
      "----------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.497   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ykite\\anaconda3\\envs\\bert_book_stockmark\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\ykite\\anaconda3\\envs\\bert_book_stockmark\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020e9a2bfb284d409ac89ff1dfc65966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification_pl(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=9,\n",
    "    lr=1e-5,\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db3042a3-d66d-4e95-8b2e-79c0c5d20614",
   "metadata": {},
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboad --logdir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a183a7b9-ca99-496c-92cc-c4679da2da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストモデルのファイル: C:\\Users\\ykite\\Documents\\projects\\study\\bert_book_stockmark\\chap6\\model\\epoch=4-step=695.ckpt\n",
      "ベストモデルの検証データに対する損失 tensor(0.4203, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_model_path = checkpoint.best_model_path\n",
    "print(\"ベストモデルのファイル:\", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証データに対する損失\", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07dc4941-780f-42c7-a61c-95b680852219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ykite\\anaconda3\\envs\\bert_book_stockmark\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at C:\\Users\\ykite\\Documents\\projects\\study\\bert_book_stockmark\\chap6\\model\\epoch=4-step=695.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at C:\\Users\\ykite\\Documents\\projects\\study\\bert_book_stockmark\\chap6\\model\\epoch=4-step=695.ckpt\n",
      "C:\\Users\\ykite\\anaconda3\\envs\\bert_book_stockmark\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89dd01d067c438ba7d63f1e6414483b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        accuracy            0.8812754154205322\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# 最後に、ファインチューニングで得たモデルをテストデータで評価\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f\"Accuracy: {test[0]['accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae6b594c-4cc3-452d-b457-505dcfc61e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightning モデルのロード\n",
    "model = BertForSequenceClassification_pl.load_from_checkpoint(\n",
    "    best_model_path,\n",
    ")\n",
    "\n",
    "# 以下のように保存\n",
    "# ├── model_transformers\n",
    "# │   ├── config.json\n",
    "# │   └── model.safetensors\n",
    "model.bert_sc.save_pretrained(\"./model_transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea83d562-6e53-4cc4-b498-fe284f4ffe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sc = BertForSequenceClassification.from_pretrained(\"./model_transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f07430f9-0ccd-4341-a536-fa83485cb3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
