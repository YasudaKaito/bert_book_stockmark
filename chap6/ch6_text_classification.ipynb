{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71d1aec-daae-422c-b741-f66fcf7e2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# [CLS] -> BertModel -> Linear -> tanh -> Linear\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "MODEL_NAME = \"tohoku-nlp/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5631f5db-798a-4d67-88f1-2999354318cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee09717-de60-4812-9796-51f3620757b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "text_list = [\n",
    "    \"この映画は面白かった。\",\n",
    "    \"この映画の最後にはがっかりさせられた。\",\n",
    "    \"この映画を見て幸せな気持ちになった。\",\n",
    "]\n",
    "label_list = [\n",
    "    1,\n",
    "    0,\n",
    "    1\n",
    "]\n",
    "\n",
    "# 符号化\n",
    "encoding: BatchEncoding = tokenizer(\n",
    "    text_list,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "labels = torch.tensor(label_list).cuda()\n",
    "\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "# 分類スコア\n",
    "# 形状は(バッチサイズ（文章数）、 カテゴリ数)\n",
    "scores = output.logits\n",
    "labels_predicted = scores.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026e3222-82ea-4398-96fd-065daf5fc7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135a8e46-0f67-4acf-8bbc-3913fdec8b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# モデルの分類器のパラメータ初期値はランダムな値のため、精度は低い\n",
    "num_correct = (labels_predicted == labels).sum().item()\n",
    "accuracy = num_correct / labels.size(0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074aa12b-c652-48bf-b5b6-fe8bf4eaab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 符号化\n",
    "encoding: BatchEncoding = tokenizer(\n",
    "    text_list,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "encoding[\"labels\"] = torch.tensor(label_list)\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "# ロスの計算\n",
    "output = bert_sc(**encoding)\n",
    "loss = output.loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe28c5-0f25-4937-aed3-ced6602cf7b4",
   "metadata": {},
   "source": [
    "OSコマンドで livedoor ニュースコーパスをダウンロード後、以下の処理につづく"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43784e45-e5ff-4cce-b2d5-784739faed84",
   "metadata": {},
   "source": [
    "## 6-5 BERT のファインチューニングと性能評価\n",
    "- データ(符号化された文章)とラベルを抜き出し、ミニバッチにする\n",
    "- データローダはデータセットからミニバッチを取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a72c833-4fa6-4679-b71d-70245beb038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "{'data': tensor([[0, 1],\n",
      "        [2, 3]]), 'labels': tensor([0, 1])}\n",
      "batch 1\n",
      "{'data': tensor([[4, 5],\n",
      "        [6, 7]]), 'labels': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "dataset_for_loader = [\n",
    "    {\n",
    "        \"data\": torch.tensor([0, 1]),\n",
    "        \"labels\": torch.tensor(0),\n",
    "    },\n",
    "    {\n",
    "        \"data\": torch.tensor([2, 3]),\n",
    "        \"labels\": torch.tensor(1),\n",
    "    },\n",
    "    {\n",
    "        \"data\": torch.tensor([4, 5]),\n",
    "        \"labels\": torch.tensor(2),\n",
    "    },\n",
    "    {\n",
    "        \"data\": torch.tensor([6, 7]),\n",
    "        \"labels\": torch.tensor(3),\n",
    "    },\n",
    "]\n",
    "loader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# ミニバッチを取り出す\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f\"batch {idx}\")\n",
    "    print(batch)\n",
    "    # ファインチューニングではここでミニバッチ毎の処理をおこなう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b22196-ea4c-48ec-9a7a-9f06f5b22bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "{'data': tensor([[4, 5],\n",
      "        [2, 3]]), 'labels': tensor([2, 1])}\n",
      "batch 1\n",
      "{'data': tensor([[0, 1],\n",
      "        [6, 7]]), 'labels': tensor([0, 3])}\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset_for_loader, batch_size=2, shuffle=True)\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f\"batch {idx}\")\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab036a-b397-49cb-8ac2-116c8c6a1571",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "各データを次のキーを持つ辞書にする\n",
    "すなわち、 tokenizer で符号化を行ったときに得られる形式\n",
    "\n",
    "- input_ids\n",
    "- attention_mask\n",
    "- token_type_ids\n",
    "- labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff5e1eb-2ce1-4c04-9751-1618915503ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:12<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "category_list = [\n",
    "    \"dokujo-tsushin\",\n",
    "    \"it-life-hack\",\n",
    "    \"kaden-channel\",\n",
    "    \"livedoor-homme\",\n",
    "    \"movie-enter\",\n",
    "    \"peachy\",\n",
    "    \"smax\",\n",
    "    \"sports-watch\",\n",
    "    \"topic-news\",\n",
    "]\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データ整形\n",
    "# 学習の高速化のため128とする\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "\n",
    "for label, category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f\"./text/{category}/{category}*\"):\n",
    "        lines = open(file, encoding=\"utf-8\").read().splitlines()\n",
    "        # 4行目からが本文\n",
    "        text = \"\\n\".join(lines[3:])\n",
    "        encoding = tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        encoding[\"labels\"] = label\n",
    "        # なぜこの場合はここでtensor化するのか? return_tensor ではだめなのか?\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325edba7-c6bf-4544-8e32-9e42b7e69870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]),\n",
      " 'input_ids': tensor([    2,  2340, 19693, 10585, 28459,    35,  6692, 28493,    13,   501,\n",
      "           62,   101,    37,     8,   569,   335,     5,    51,     7,     9,\n",
      "         1040,     5,   616,     9,  2941,    18,  5602,   501,    20,    16,\n",
      "         4027, 10531,   140,    36,    73, 30020, 28457, 25127,    38,  1080,\n",
      "            5,    53,    28,   707,     5,    12,     9,    80,  3635,   205,\n",
      "           29,  2935,   604,  5846,  6503,    11,  4722,    16,   861,    13,\n",
      "            6, 12272, 24050,  2079,    11,    26,    62,    45,    28,  2451,\n",
      "           80,     8,    36, 24050,    14,    31,  1058,    75, 11218, 10531,\n",
      "         3676,   542,     5, 22130,     6,  5408,    16,  4831,    80,    29,\n",
      "           18,  7045,    26, 28456,  4799,   900,     6,   569,   335,     9,\n",
      "         1704,  1277,    15,  3318,  2575,    29,  2935,  5233,    75,    13,\n",
      "         3472,   459,    12,  8585,  3171,   312,  3676,   542, 22130,   241,\n",
      "            5,   709, 28696,  2180,    14, 12959,    71,     3]),\n",
      " 'labels': tensor(0),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "7367\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset_for_loader[0])\n",
    "print(len(dataset_for_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3cbd1-59f6-41fc-900e-0ecf8ae6a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット分割\n",
    "# 学習/検証/テスト = 6:2:2 で分割\n",
    "random.shuffle(dataset_for_loader)\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "\n",
    "dataset_train = dataset_for_loader[:n_train]\n",
    "dataset_val = dataset_for_loader[n_train: n_train + n_val]\n",
    "dataset_test = dataset_for_loader[n_train + n_val:]\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# 学習はshuffle=True\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    # BERTのオリジナルの論文を参考に32としているとのこと\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    # 損失の勾配を計算しないので、大きめのバッチサイズ\n",
    "    batch_size=256,\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=256,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
